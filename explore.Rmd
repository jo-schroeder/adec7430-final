---
title: "Explore the Nominees Model"
output: 
  html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, echo = FALSE
)

# loading packages
library(lubridate)
library(dtwclust)
library(ggthemes)
library(ggplot2)
library(plotly)
library(dplyr)
library(tidyr)
library(tsfeatures)
library(tidymodels)
library(purrr)
library(readr)
library(stringr)
library(caret)
library(randomForest)
library(tibble)
library(kableExtra)
library(themis)
library(corrplot)

# load in data
data_dir <- "./data/"
actors <- read_csv(paste0(data_dir, "actors_features.csv"))
trends <- read_csv(paste0(data_dir, "trends.csv"))

# summary stats
summary(actors)
# Some of the actor info of interest has NA values (15 birthdays, 5 Popularity scores), so we will have to decide what we want to do about those
summary(trends)
# <1 trend needs to be recoded as numeric, I think we should choose 0 
# also the trends were downloaded for actors who were in popular movies in 2025, but they won't be included in the analysis
```

## Actor Features

Before jumping into modeling, it is best to perform exploratory data analysis. Exploratory data analysis helps us to better understand our dataset. And when we understand our dataset we make better decisions about how best to model the phenomenon. Since we are trying to predict Oscar nominations with this model, we explored how the variables we’ve collected relate to Oscar nomination. 

We started by collecting summary statistics and missingness for our dataset. We visualize some of the summary trends below. As for missingness, we found that quite a few variables had some missing values. In most cases, missingness was caused information from The Movie Database (TMDB) was not found for each actor. For example, Peter O’Toole, who was nominated for an Oscar in 2006, had no information matched in TMDB. This is because the [first match for ‘Peter O’Toole’ in TMDB](https://www.themoviedb.org/search/person?language=en-US&query=Peter%20O%27Toole) is a Peter O’Toole who works on movies in Lighting. In the future we would like to refine this dataset performing more manual checks to account for missingness and augment the existing data, but for now, due to the random nature of the missingness, we filter out observations with missingness when necessary. 


```{r}
# Exploring birth years of actors
actors %>% 
  filter(birth_year >=1920) %>%
  ggplot(aes(x = birth_year, color = factor(nominee))) +
  geom_density() +
  labs(title = "When were Oscar Nominated and Commercially Successful Actors Born?",
       subtitle = "Actors nominated for an Oscar or in a commercially successful movie from 2004 - 2024",
       x = "Birth Year",
       y = "Proportion",
       caption = "Data Sources: The Oscar Award, 1927-2025 (aquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API)",
       color = "") +
  scale_color_solarized(labels = c("Non-Nominee", "Nominee")) +
  theme_minimal() +
  guides(color = guide_legend(override.aes = list(fill = c("#268bd2", "#dc322f")))) 
```

To start, we looked at the birth year of actors in our dataset by nomination. We see different patterns in the distribution of birth years for nominees and non-nominees. Non-nominees are more likely to be born in later years, suggesting that, on average, commercially successful movies cast younger actors. This pattern may also suggest that actors don’t reach critical acclaim until they are older and more established in their careers.

```{r}
# by gender
actors %>% filter(Gender != "Not set") %>%
  ggplot(aes(x = Gender, fill = factor(nominee))) +
  geom_histogram(stat="count", position = "dodge") +
  labs(title = "What Gender are Oscar Nominated and Commercially Successful Actors?",
       subtitle = "Actors nominated for an Oscar or in a commercially successful movie from 2004 - 2024",
       x = "Gender",
       y = "Count",
       caption = "Data Sources: The Oscar Award, 1927-2025 (aquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API)",
       fill = "") +
  scale_fill_solarized(labels = c("Non-Nominee", "Nominee")) +
  theme_minimal()
# This is sort of interesting- there's 10 male and 10 female actors nominated every year. This suggests that men born after 1975 are less represented in nominations overall, esp. compared to women in the same age group. Dominance of older actors in male categories?
```

Next, we looked into differences in the gender of actors by nomination. The number of male and female nominees is identical, which makes sense given that The Academy’s acting awards split along a gender binary. Ten female and ten male actors are nominated each year. It is surprising, on the other hand, that there are far more male actors in commercially successful movies than female. This suggests that commercially successful movies are more likely to have male cast members. 

```{r}
actors %>%
  ggplot(aes(x = american, fill = factor(nominee))) +
  geom_histogram(stat="count", position = "dodge") +
  labs(title = "What Nationality are Oscar Nominated and Commercially Successful Actors?",
       subtitle = "Actors nominated for an Oscar or in a commercially successful movie from 2004 - 2024",
       x = "Nationality",
       y = "Count",
       caption = "Data Sources: The Oscar Award, 1927-2025 (aquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API)",
       fill = "") +
  scale_fill_solarized(labels = c("Non-Nominee", "Nominee")) +
  scale_x_continuous(breaks = c(0, 1), labels = c("Non-American", "American")) +
  theme_minimal()
```

We also look at the nationality of actors by nomination. We find that American actors are more likely to be nominated for an Academy Award than non-Americans. While this trend is likely indicative of patterns in Hollywood, it also reflects how we defined commercial success. When we pulled commercially successful movies from TMDB, we did not limit the results to a specific region. So, our dataset includes many actors who starred in internationally successful movies that may not have been popular with American audiences or critics. In the future we may think about limiting the scope of how we define commercial success to improve the model performance. Although, including more international stars in our analysis also demonstrates the blind spots Hollywood critics have to non-Western films and actors. 

```{r}
# exploring age at nomination
actors %>% 
  filter(nominee == 1) %>%
  mutate(min_year = map_dbl(nominated_years, function(s) {
    s %>%
      str_remove_all("\\[|\\]") %>%       
      str_split(",\\s*") %>%              
      unlist() %>%
      as.numeric() %>%
      min(na.rm = TRUE)
  })) %>%
  mutate(age_at_nomination = as.numeric(difftime(min_year, birth_year))) %>%
  filter(age_at_nomination < 100, Gender != "Non-binary/Other") %>%
  ggplot(aes(x = age_at_nomination, color = Gender)) +
  geom_density() +
  labs(title = "At What Age are Oscar Nominated Actors Nominated?",
       subtitle = "Actors nominated for an Oscar from 2004 - 2024",
       x = "",
       y = "Proportion",
       caption = "Data Sources: The Oscar Award, 1927-2025 (aquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API)",
       fill = "") +
  scale_color_solarized(labels = c("Female", "Male", NA)) +
  theme_minimal() +
  guides(color = guide_legend(override.aes = list(fill = c("#268bd2", "#dc322f")))) +
  scale_x_continuous(breaks = c(20, 30, 40, 50, 60, 70, 80))
# Obviously there's an error here - I'll have to go back and check the API call results for the 130 year old but I'm guessing there's another famous person with the same name

```

Finally, we looked at the age at nomination for Oscar nominated actors by gender. This summary demonstrates the potential interaction effects of some our variables, in this case, age and gender. We see that female nominees are more likely to be younger, peaking in their mid-30s. Women also experience a sharp dropoff in nominations in their mid-40s. Men, on the other hand, reach their peak years in their early 40s, on average, and experience a more graduated dropoff in their careers.

## Google Trends

```{r}
# interactive trends plot
p <- plot_ly(type = "scatter", mode = "lines")

trends <- trends %>%
  mutate(across(everything(), ~ ifelse(. == "<1", 0, .))) %>%
  mutate(across(c("Mark Ruffalo":"Aaron Eckhart"), ~ as.numeric(.))) %>%
  pivot_longer(cols = c("Mark Ruffalo":"Aaron Eckhart")) %>%
  mutate(Month = ym(Month))

trends %>%
  group_by(name) %>%
  group_split() %>%
  purrr::walk(function(df) {
    p <<- add_trace(p,
                    data = df,
                    x = ~Month,
                    y = ~value,
                    name = unique(df$name),
                    text = ~paste(name, value),
                    hoverinfo = "text",
                    visible = "legendonly")
  })

p %>%
  layout(
    title = "Monthly Search Interest for Selected Actors since 2004",
    xaxis = list(title = "Month", range = c("2004-01-01",
                                            "2025-03-01")),
    yaxis = list(title = "Google Trend Search Interest", range = c(0, 100)),
    legend = list(title = list(text = "Select an Actor"))
  )
```

In addition to the actor features, we relied on Google trends for data in our modeling strategy. The interactive plot allows you to explore the Google trend for actors in the dataset. 

```{r}

# joining 
joined <- trends %>% 
  mutate(name = tolower(name)) %>%
  left_join(actors, by = c("name")) %>%
  mutate(type = factor(nominee, levels = c(0, 1),
  labels = c("Commercially Successful", "Oscar Nominated")))

# by winner
joined %>% 
  filter(!is.na(type)) %>%
  group_by(Month, type) %>% summarise(value = mean(value)) %>%
  ggplot(aes(x = Month, y = value, color = factor(type))) +
  geom_line() +
  labs(title = "What are the patterns in Google Trend Search Interest for Actors?",
       subtitle = "Actors nominated for an Oscar or in a commercially successful movie from 2004 - 2024",
      caption = "Data Sources: The Oscar Award, 1927-2025 (aquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API),\nGoogle Trends",
      x = "", y = "Google Trend Search Interest (mean)") +
  facet_wrap(vars(type), ncol = 1) +
  theme_minimal() +
  scale_color_solarized() +
  theme(legend.position = "none")
# This is pretty interesting - suggests that generally Oscar nominated actors are have searched more but it has leveled off (could reflect trends in Google Search overall). The cyclical pattern is also interesting and I'm guessing it coincides with the ceremony and nomination announcements!
```

To understand the overall patterns in Google Trends for our groups of interest, we aggregated all trends to find the mean values for Search Interest. We find that the Google Trends of commercially successful and Oscar nominated actors can be characterized differently. Oscar nominated actors, in aggregate, experience more extreme trends in their search interest throughout the year. In fact, the trend for Oscar nominated actors tracks pretty much exactly in line with when Oscar nominations are announced and when the awards show occurs. Both trends display the phenomenon of Google Search becoming more popular until the mid 2010s when search starts to level off overall.

```{r}
table <- joined %>% mutate(month = month(Month)) %>%
  filter(!is.na(type)) %>%
  group_by(type, month) %>%
  summarise(mean_value = mean(value)) %>%
  ungroup(month) %>%
  slice_max(order_by = mean_value, n = 3) %>%
  mutate(mean_value = round(mean_value, 1))

table %>%
  kable(caption = "Data Sources: The Oscar Award, 1927-2025 (acquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API),\nGoogle Trends",
        col.names = c("Type", "Month", "Google Trend Search Interest (mean)")) %>%
  add_header_above(c("Three Most Popular Months for Google Search Interest by Actor Type" = 3)) %>%
  kable_styling(full_width = FALSE)
```

Indeed, when we look at the most popular months to search each type of actor, commercially successful actors experience the most traffic in January, July, and December, when big blockbuster movies typically release. Oscar nominated actors, on the other hand, experience the most traffic in January, February, and March, during awards season. The descriptive differences we find in the trends between these two groups leads us to believe that, based on information from the time series alone, we may be able to distinguish between these two groups. 

### Clustering

Because I have a methodological interest in clustering, I was interested to explore a clustering solution for the Google Trends of our actors. Typically, time series clustering can take two approaches: shape-based clustering and feature-based clustering. For this phase of analysis, we decided to use shape-based clustering, and we explore extracting features from time series later in the project. After trying several clustering methods and options for distance, we chose to take a partitional clustering approach with shape-based distance, using the `tsclust` package in R. The Average Silhouette Width, a metric for cluster quality, for our clusters was never within Kaufman and Rousseeuw’s proposed interpretation for ASW as having any kind of discernible structure. [^1] Keeping this in mind, we would like to engineer a better shape-based clustering solution in the future, but for now we propose the following five cluster solution. 

```{r out.width="100%"}
set.seed(90210)
wide <- trends %>%
  pivot_wider(names_from = Month, values_from = value) %>%
  tibble::column_to_rownames("name")
ts_matrix <- as.matrix(wide)

clusters <- tsclust(ts_matrix,
                    type = "partitional",
                    k = 5,  
                    distance = "sbd",
                    centroid = "shape",  # required with SBD
                    control = partitional_control(iter.max = 50),
                    seed = 90210)

cluster_assignments <- clusters@cluster  # cluster labels for each series
distances <- clusters@cldist

# Create a data frame with rownames and cluster assignment
cluster_df <- data.frame(
  name = rownames(ts_matrix),
  cluster = cluster_assignments,
  distance = distances
)

actor_with_cluster <- left_join(trends, cluster_df, by = "name") %>%
  mutate(cluster_label = case_when(
    cluster == 1 ~ paste0(cluster, "\nBlockbuster Staple"),
    cluster == 2 ~ paste0(cluster, "\nBreakout Star or\nMemorialized"),
    cluster == 3 ~ paste0(cluster, "\nCritically Acclaimed"),
    cluster == 4 ~ paste0(cluster, "\nPop Fame"),
    cluster == 5 ~ paste0(cluster, "\nEmerging or\nConsistent Stardom")
  ))

actor_with_cluster %>%
  group_by(Month, cluster_label) %>% summarise(value = mean(value)) %>%
  ggplot(aes(x = Month, y = value, colour = factor(cluster_label))) +
  geom_line(show.legend = FALSE) +
  scale_x_date(breaks = as.Date(c("2000-01-01", "2010-01-01", "2020-01-01")),
  date_labels = "%Y") +
  facet_wrap(vars(cluster_label), ncol = 5) + 
  scale_color_solarized() +
  theme_minimal() +
  labs(title = "5 Cluster Solution for Actor Google Search Trends",
       x = "", y = "Google Trend Search Interest (mean)")
```

The first cluster, Blockbuster Staples, are characterized by actors who experience a consistent growing or high level of popularity. Within the cluster centroids, this group experiences no major peaks, signifying that they have a relatively steady level of Search Interest. Chris Hemsworth, Nicholas Cage, Sam Elliot, and Mads Mikkelsen, who are all representative members of this cluster, have starred in Marvel franchise movies as well as numerous other blockbuster projects throughout this time period. 

The second cluster, Breakout Star or Memorialized, are characterized by actors with the sharpest peaks in their Search Interest. Two of the representative members of this cluster, Maria Bakalova (*Borat Subsequent Moviefilm* - 2020) and Jean Dujardin (*The Artist* - 2011), had breakthrough roles that earned them Academy Award Nominations. Irrfan Khan, as well, earned critical acclaim in Indian film awards. Chadwick Boseman and Carrie Fisher both had high profile deaths during this time period. This demonstrates the limitations of this kind of clustering method, as our data cannot on its own does not discern between interest due to breakout success, a tragic death, or a scandal. Although, it should be noted that Chadwick Boseman was nominated for an Academy Award (*Ma Rainey’s Black Bottom* - 2020) in the year of his death. This is the cluster with the highest membership, which could underscore the unruliness of its interpretation.

```{r out.width="100%"}
centroid_matrix <- do.call(rbind, clusters@centroids)

# Create a data frame from the matrix
Time <- rep(seq.Date(from = as.Date("2004-01-01"), 
                     to = as.Date("2025-03-01"), 
                     by = "month"), 
            times = nrow(centroid_matrix))

centroid_df <- data.frame(Time = Time,
                          Cluster = rep(1:nrow(centroid_matrix), 
                          each = ncol(centroid_matrix)),
                          Value = as.vector(centroid_matrix)) %>%
  mutate(cluster_label = case_when(
    Cluster == 1 ~ paste0(Cluster, "\nBlockbuster Staple"),
    Cluster == 2 ~ paste0(Cluster, "\nBreakout Star or\nMemorialized"),
    Cluster == 3 ~ paste0(Cluster, "\nCritically Acclaimed"),
    Cluster == 4 ~ paste0(Cluster, "\nPop Fame"),
    Cluster == 5 ~ paste0(Cluster, "\nEmerging or\nConsistent Stardom")
  ))


centroid_df %>%
  ggplot(aes(x = Time, y = Value, color = factor(cluster_label))) +
    geom_line() +
    facet_wrap(~ cluster_label, ncol = 5) +
    labs(title = "Cluster Centroids", x = "Time", y = "Value", color = "Cluster") +
    theme_minimal() +
    scale_x_date(breaks = as.Date(c("2000-01-01", "2010-01-01", "2020-01-01")),
    date_labels = "%Y") +
    scale_color_solarized() +
    theme_minimal() +
    labs(title = "Cluster Centroids for Actor Google Search Trends",
         x = "", y = "Google Trend Search Interest (Normalized)") +
  theme(legend.position = "none")
```

The third cluster, Critically Acclaimed, also includes representative actors who have been nominated for Academy Awards, including Felicity Jones (*Theory of Everything* - 2014) and Remi Malek (*Bohemian Rhapsody* - 2018). Members Tom Hiddleston and Glen Powell have been nominated for or won Golden Globe awards, and Pedro Pascal is an acclaimed star of the silver screen. These actors may be characterized by relatively higher recent Search Interest, as well.

The fourth cluster, Pop Fame, is characterized by actors who achieved fame in popular franchises of the mid-2000s, like Taylor Lautner in the *Twilight* movies, Orlando Bloom in *Pirates of the Caribbean*, Megan Fox in *Transformers*, and Moon Bloodgood in *Terminator Salvation*. Frieda Pinto has received critical praise for her roles, including in *Slumdog Millionaire*. Many of these actors have likely fallen off relative to their popularity at their respective peaks. This group has the lowest membership. 

```{r out.width="100%"}
actor_with_cluster <- actor_with_cluster %>% 
  mutate(cluster_label = str_replace(cluster_label, "\n", " - ")) %>%
  mutate(cluster_label = str_replace(cluster_label, "\n", " "))

table <- actor_with_cluster %>% 
  distinct(name, cluster_label, distance) %>%
  group_by(cluster_label) %>%
  slice_min(order_by = distance, n = 5) %>%
  mutate(row = row_number()) %>%         
  select(cluster_label, row, name) %>%  
  pivot_wider(names_from = cluster_label, values_from = name) %>%
  select(-row)

table %>%
  kable(caption = "Data Sources: The Oscar Award, 1927-2025 (acquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API),\nGoogle Trends") %>%
  add_header_above(c("Five Most Representative Actors in Each Cluster" = 5)) %>%
  kable_styling(full_width = FALSE)
```

Lastly, the fifth cluster, Emerging or Consistent Stardom, are folks who have more middling search interest over time. Many of them have been working steadily, including the representatives of this cluster, and have seen both franchise and critical success, including the representative of this cluster. These actors are likely more middling in their Search Interest, experiencing more consistent buzz or a slight uptick over time.

```{r out.width="100%"}
table <- actor_with_cluster %>% distinct(name, cluster_label) %>%
  mutate(name = tolower(name)) %>%
  left_join(actors) %>%
  filter(!is.na(cluster_label)) %>%
  group_by(cluster_label) %>% mutate(total = n()) %>%
  group_by(cluster_label, nominee) %>%
  summarise(prop_nominees = round((n()/total)*100)) %>%
  filter(nominee == 1) %>% ungroup(nominee) %>% select(-nominee) %>%
  distinct()

table %>%
  kable(caption = "Data Sources: The Oscar Award, 1927-2025 (acquired from Kaggle),\nFilm and Actor Details (acquired from The Movie Database API),\nGoogle Trends",
        col.names = c("Cluster", "Percent Nominees (%)")) %>%
  add_header_above(c("What Percent of Actors in Each Cluster are Oscar Nominees?" = 2)) %>%
  kable_styling(full_width = FALSE)
```

By creating these clusters, we hope to not only describe patterns in actor search interest, but test if these patterns have explanatory power. Two of our clusters, Breakout Stars and Memorialized and Critically Acclaimed, have a higher proportion of Oscar nominees than the other clusters. This suggests that our clusters may have descriptive significance, despite not being a strong solution technically. To further explore this idea, we use the cluster as a feature in our predictive model. 

## Predictive Modeling

In addition the features explored earlier, we derived features from the time series for each calendar year for each actor. The outcome is nomination in the next year's award ceremony, but for simplicity we describe this as occurring within the same year. We tested out using longer time series, for example, two years (which would include buzz generated while a movie was in production, for example), but we had issues with sparsity. Some actors were not prominent for several years before blowing up in popularity (i.e., several consecutive months with no interest), which can make feature extraction impossible. So, we filtered out observations with no variation in search interest. 

Deriving time series features involves summarizing the properties of a time series, and we used the `tsfeatures` R package to accomplish this. `tsfeatures` extracts variables like linearity, curvature, trend, entropy, autocorrelation, and spikiness. [^2] In addition to the extracted features, we also engineered a summary feature of our own: `max_spike_height`. This captures the maximum search interest for a calendar year, which would show if they reached the relative peak of their search interest during this time period. 

```{r}
library(knitr)

features <- c(
  "trend", "spike", "linearity", "curvature", "e_acf1", "e_acf10",
  "entropy", "x_acf1", "x_acf10", "diff1_acf1", "diff1_acf10", "diff2_acf1",
  "max_spike_height", "nominated_previously", "age", "gender", "american",
  "cluster", "won_previously"
)

model1 <- features
model2 <- setdiff(features, c("nominated_previously", "age", "gender", "american", "won_previously"))

df <- data.frame(
  Feature = features,
  `Model 1 - All Features` = ifelse(features %in% model1, "✔", ""),
  `Model 2 - TS Features Only` = ifelse(features %in% model2, "✔", "")
)

df %>%
  kable(col.names = c("Feature", "Model 1 - All Predictors", "Model 2 - TS Predictors Only")) %>%
  add_header_above(c("Model Feature Comparison" = 3)) %>%
  kable_styling(full_width = FALSE)
```

To test our hypothesis about the role of Oscar buzz in predicting nominations, we wanted to build two models. The first model would include all of the features we had engineered, and the second would include only features derived from the Google Trend. Comparing how these two models perform would help us understand the explanatory power of Oscar hype. 


```{r, include = FALSE}
years_seq <- 2004:2024

cluster_df <- cluster_df %>%
  distinct(name, cluster) %>%
  mutate(name = tolower(name))

features <- actors %>% 
  left_join(cluster_df, by = ("name")) %>% 
  select(name, age, gender, american, cluster)

winners <- actors %>%
  mutate(year_winner = str_remove_all(year_winner, "\\[|\\]")) %>%
  separate_rows(year_winner, sep = ",\\s*") %>%             
  mutate(year_winner = as.integer(year_winner)) %>%
  select(name, winner, year_winner)

# Cross join name and years_seq
winners_filled <- winners %>%
  # Create a data frame of all names and years
  distinct(name) %>%
  expand(name, year_winner = years_seq) %>%
  left_join(winners, by = c("name", "year_winner")) %>%
  rename(year = year_winner) %>%
  group_by(name) %>%
  mutate(
    won = ifelse(is.na(winner), "no", winner)
  ) %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    won_previously = ifelse(lag(won != "no", default = FALSE), TRUE, FALSE)) %>%
  ungroup() %>%
  mutate(
    won_previously = ifelse(won_previously == TRUE, 1, 0)
  ) %>%
  group_by(name) %>%
  # Fill the 'nominated_previously' column down for each name
  mutate(won_previously = cummax(won_previously)) %>% 
  ungroup() %>%
  select(-won)

nominees <- actors %>%
  mutate(nominated_years = str_remove_all(nominated_years, "\\[|\\]")) %>%
  separate_rows(nominated_years, sep = ",\\s*") %>%             
  mutate(nominated_years = as.integer(nominated_years)) %>%
  select(name, nominated_years) %>%
  mutate(nominee = ifelse(!is.na(nominated_years), "yes", "no"))

nominees_filled <- nominees %>%
  # Create a data frame of all names and years
  distinct(name) %>%
  expand(name, nominated_years = years_seq) %>%
  left_join(nominees, by = c("name", "nominated_years")) %>%
  rename(year = nominated_years) %>%
  group_by(name) %>%
  mutate(
    nominated = ifelse(is.na(nominee), "no", nominee)  # Ensure no NAs in 'nominated' column
  ) %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    nominated_previously = ifelse(lag(nominated == "yes", default = FALSE), TRUE, FALSE)
  ) %>%
  ungroup() %>%
  mutate(
    nominated_previously = as.integer(nominated_previously)
  ) %>%
  group_by(name) %>%
  # Fill the 'nominated_previously' column down for each name
  mutate(nominated_previously = cummax(nominated_previously)) %>% 
  ungroup() %>%
  select(-nominated)

winners_filled <- winners_filled %>%
  left_join(features, by = "name") %>%
  mutate(age = year - age)
prev_win <- winners_filled %>%
  select(name, year, won_previously)
nominees_filled <- nominees_filled %>%
  left_join(features, by = "name") %>%
  left_join(prev_win, by = c("name", "year")) %>%
  mutate(age = year - age)


trend_ts_data_all <- trends %>%
  mutate(year = year(Month),
         name = tolower(name)) %>%
  group_by(name, year) %>%
  arrange(Month) %>%
  summarise(
    ts_data = list(ts(value, frequency = 12)),
    var = var(value, na.rm = TRUE),
    .groups = "drop"
  )

trend_ts_data <- trend_ts_data_all %>%
  filter(!is.na(var) & var > 0) %>%
  select(-var) %>%
  filter(year != 2025)

# feature extraction + custom max spike height
trend_features_ts <- trend_ts_data %>%
  mutate(
    features = map(ts_data, ~ tsfeatures(.x)),  
    max_spike_height = map_dbl(ts_data, ~ max(.x, na.rm = TRUE))
  ) %>%
  unnest(features)

model_data <- trend_features_ts %>%
  left_join(nominees_filled, by = c("name", "year")) %>%
  mutate(
    nominee = factor(ifelse(is.na(nominee), 0, 1))  
  ) %>%
  select(-frequency, -nperiods, -seasonal_period, -diff2_acf10, -seas_acf1) %>%
  filter(entropy != 1)
```

Ours is a classification problem: in a given year, will an actor be nominated for an Oscar? After testing a few different modeling strategies, including logistic regression, we decided to use Random Forest. Random Forest has many strengths as a machine learning model, but the one that was arguably most important for our purposes is to handle high-dimensional, noisy data. The first hurdle to overcome with Random Forest was to handle our large class imbalance. Our initial models were not very accurate due to the relatively low occurrence of nominees. In a given year only 20 actors are nominees, and several have been nominated multiple times, which doesn’t yield many observations for training the model. So we decided to undersample non-nominees to achieve a balanced training dataset. Once we started seeing more accurate predictions, we added a 5 fold cross validation to cover potential blindspots introduced by undersampling and provide more stable evaluation metrics. Cross validation allows us to be more certain about our model’s performance.

```{r}
set.seed(90210)
model_data1 <- model_data %>%
  mutate(across(c(cluster, gender, american, won_previously, nominated_previously), as.factor)) %>%
  drop_na() %>%
  mutate(row_id = row_number())
  
# Save for reconnecting later
id_lookup <- model_data1 %>% select(row_id, name, year)

# Drop unused columns and handle factors
model_data1 <- model_data1 %>% select(-name, -ts_data, -year, -row_id)
model_data2 <- model_data1 %>% select(-age, -gender, -american, -nominated_previously, -won_previously)

# Define a function to build a workflow with downsampling and cross-validation
define_modeling_workflow <- function(data) {
  # Split data
  data_split <- initial_split(data, prop = 0.8, strata = nominee)
  train_data <- training(data_split)
  test_data <- testing(data_split)

  # Cross-validation folds
  folds <- vfold_cv(train_data, v = 5, strata = nominee)

  # Recipe with downsampling
  rf_recipe <- recipe(nominee ~ ., data = train_data) %>%
    step_downsample(nominee)

  # Random forest model
  rf_model <- 
    rand_forest(mode = "classification") %>%
    set_engine("randomForest")

  # Workflow
  rf_workflow <- workflow() %>%
    add_model(rf_model) %>%
    add_recipe(rf_recipe)

  list(
    workflow = rf_workflow,
    folds = folds,
    test_data = test_data
  )
}

# Run for all predictors
workflow_all <- define_modeling_workflow(model_data1)
rf_results_all <- fit_resamples(
  workflow_all$workflow,
  resamples = workflow_all$folds,
  metrics = metric_set(accuracy, roc_auc),
  control = control_resamples(save_pred = TRUE)
)
final_model_all <- last_fit(
  workflow_all$workflow,
  split = initial_split(model_data1, prop = 0.8, strata = nominee)
)
fitted_rf_all <- extract_fit_parsnip(final_model_all)$fit

# Run for time series predictors only
workflow_ts <- define_modeling_workflow(model_data2)
rf_results_ts <- fit_resamples(
  workflow_ts$workflow,
  resamples = workflow_ts$folds,
  metrics = metric_set(accuracy, roc_auc),
  control = control_resamples(save_pred = TRUE)
)

final_model_ts <- last_fit(
  workflow_ts$workflow,
  split = initial_split(model_data2, prop = 0.8, strata = nominee)
)
fitted_rf_ts <- extract_fit_parsnip(final_model_ts)$fit

collect_metrics(rf_results_all) %>% select(-.estimator, -.config) %>%
  kable(digits = 2, col.names = c("Metric", "Mean", "n Folds", "Standard Error")) %>%
  add_header_above(c("Model Accuracy - All Predictors" = 4)) %>%
  kable_styling(full_width = FALSE)
collect_metrics(rf_results_ts) %>% select(-.estimator, -.config) %>%
  kable(digits = 2, col.names = c("Metric", "Mean", "n Folds", "Standard Error")) %>%
  add_header_above(c("Model Accuracy - TS Predictors Only" = 4)) %>%
  kable_styling(full_width = FALSE)
```

According to basic metrics of model accuracy, we found that the two models performed extremely similarly. Both models have an accuracy of around 70% and an ROC AUC around 0.8, which is much better than a coin flip but not remarkably excellent. Low standard error on our accuracy metrics suggest that these estimates are reliable. Model 1 - All Features slightly outperformed, but the results still suggest that Oscar buzz is a powerful explainer of Oscar nomination. 

```{r}
final_preds_all <- collect_predictions(final_model_all)  # includes `.row` and `.pred_class`
final_preds_ts <- collect_predictions(final_model_ts)

# Confusion matrices
conf_mat_all <- conf_mat(final_preds_all, truth = nominee, estimate = .pred_class)
conf_mat_ts <- conf_mat(final_preds_ts, truth = nominee, estimate = .pred_class)

# Convert to data frame
cm_all_tbl <- as.data.frame(conf_mat_all$table) %>%
  mutate(model = "All Predictors")
cm_ts_tbl <- as.data.frame(conf_mat_ts$table) %>%
  mutate(model = "TS Predictors Only")

# Combine and reshape
cm_combined <- bind_rows(cm_all_tbl, cm_ts_tbl) %>%
  pivot_wider(names_from = model, values_from = Freq) %>%
  rename(True_Label = Truth, Predicted_Label = Prediction)

# Display confusion matrix table
cm_combined %>%
  kable(col.names = c("True Label", "Predicted Label", "All Predictors", "TS Predictors Only")) %>%
  add_header_above(c(" " = 2, "Confusion Matrix Model Comparison" = 2)) %>%
  kable_styling(full_width = FALSE)
```

Looking deeper into Type I (false positive) and Type II (false negative) error for the final Random Forest model, we find that Model 2 - TS Features Only makes less Type II error and more Type I error compared to Model 1. 


```{r}
# Model metrics
metrics <- metric_set(f_meas, sens, bal_accuracy)

metrics_all <- metrics(final_preds_all, truth = nominee, estimate = .pred_class, event_level = "second") %>%
  mutate(model = "All Predictors")
metrics_ts <- metrics(final_preds_ts, truth = nominee, estimate = .pred_class,
                      event_level = "second") %>%
  mutate(model = "TS Predictors Only")

# Combine and pivot
metrics_combined <- bind_rows(metrics_all, metrics_ts) %>%
  select(.metric, model, .estimate) %>%
  pivot_wider(names_from = model, values_from = .estimate)

# Display model performance metrics
metrics_combined %>%
  kable(digits = 3, col.names = c("Metric", "All Predictors", "TS Predictors Only")) %>%
  add_header_above(c(" " = 1, "Model Performance Comparison" = 2)) %>%
  kable_styling(full_width = FALSE)

```

Although Model 2 makes less Type II error, it is far less balanced than Model 1. Model 1 has higher sensitivity and a better F1 metric than Model 2. Again, both models are not excellent, but fair okay in terms of metrics. Looking deeper into the actors misclassified may help us understand how to engineer better features for prediction. 


```{r}
# Reconnect original identifiers
final_preds_with_ids_all <- final_preds_all %>%
  left_join(id_lookup, by = c(".row" = "row_id")) %>%
  mutate(model_type = "All Predictors")

final_preds_with_ids_ts <- final_preds_ts %>%
  left_join(id_lookup, by = c(".row" = "row_id")) %>%
  mutate(model_type = "TS Predictors Only")

# Combine for comparison
all_preds_combined <- bind_rows(final_preds_with_ids_all, final_preds_with_ids_ts)

# Identify misclassifications
misclassified <- all_preds_combined %>%
  mutate(
    misclassified = case_when(
      .pred_class == 1 & nominee == 0 ~ "false positive",
      .pred_class == 0 & nominee == 1 ~ "false negative",
      TRUE ~ "true"
    )
  )

# Top false negatives by model type
fn_misclassified <- misclassified %>%
  filter(misclassified == "false negative") %>%
  group_by(model_type) %>%
  slice_max(order_by = .pred_0, n = 5, with_ties = FALSE) %>%
  mutate(row = row_number()) %>%
  ungroup() %>%
  select(model_type, name, year, row) %>%
  mutate(name = str_to_title(name)) %>%
  mutate(name_year = paste0(name, " (", year, ")")) %>%
  select(model_type, row, name_year) %>%
  pivot_wider(names_from = model_type, values_from = name_year) %>% select(-row)

# Display table
fn_misclassified %>%
  kable(col.names = c("All Predictors", "TS Predictors Only")) %>%
  add_header_above(c("Who are the Dark Horse Nominees?" = 2)) %>%
  kable_styling(full_width = FALSE)
```

False negatives can be thought of as Dark Horse nominees. These are the actors with the largest incorrect threshold for non-nominee. Interestingly, they are overwhelmingly male. This suggests that adding some additional information to the modeling strategy, such as the fact that there can be only 10 male and 10 female nominees per year, might improve our ability to detect Dark Horses. 


```{r}
# Top false negatives by model type
fp_misclassified <- misclassified %>%
  filter(misclassified == "false positive") %>%
  group_by(model_type) %>%
  slice_max(order_by = .pred_1, n = 5, with_ties = FALSE) %>%
  mutate(row = row_number()) %>%
  ungroup() %>%
  select(model_type, name, year, row) %>%
  mutate(name = str_to_title(name)) %>%
  mutate(name_year = paste0(name, " (", year, ")")) %>%
  select(model_type, row, name_year) %>%
  pivot_wider(names_from = model_type, values_from = name_year) %>% select(-row)

# Display table
fp_misclassified %>%
  kable(col.names = c("All Predictors", "TS Predictors Only")) %>%
  add_header_above(c("Who are the Oscar Snubs?" = 2)) %>%
  kable_styling(full_width = FALSE)
```

False positives can be thought of as Oscar snubs.  These are the actors with the largest incorrect threshold for nominee. In this area, adding constraints on the model so that actors must be in a significant film in the year, not in a popular TV show or having a scandale, for example, could improve our ability to detect quality Oscar snubs.

```{r}
# feature importance plots

# Model 1
importance(fitted_rf_all) %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  arrange(desc(MeanDecreaseGini)) %>%
  ggplot(aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "#268bd2") +
  coord_flip() +
  labs(title = "Feature Importance of Model 1 - All Predictors", x = "", y = "Mean Decrease in Gini") +
  theme_minimal()
```

Diving into feature importance, we see additional evidence that Google Trends derived predictors are the most influential in our models. The only non-time series variable with notable influence is age, which tracks with our findings from exploratory data analysis. Interestingly, previous Oscar wins and nominations are not very influential, which suggests that additional feature engineering to understand actor quality might not be fruitful.

```{r}
# Model 2
importance(fitted_rf_ts) %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  arrange(desc(MeanDecreaseGini)) %>%
  ggplot(aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "#dc322f") +
  coord_flip() +
  labs(title = "Feature Importance of Model 2 - Time Series Predictors Only", x = "", y = "Mean Decrease in Gini") +
  theme_minimal()
```

Within Model 2 predictors, our engineered cluster is not very influential either, which suggests again that the solution could use fine-tuning to achieve more predictive power.

```{r out.width="100%"}
numeric_vars <- model_data1 %>%
  select(where(is.numeric))
cor_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.cex = .7, tl.col = "black", 
         addCoef.col = "black", number.cex = 0.5,
         title = "Correlation for Numeric Variables",
         diag=FALSE,  mar=c(0,0,1,0))
```

Because correlated features can be washed out in feature importance, we looked at correlation of numeric features to determine if we’re overlooking any important patterns. Linearity and curvature jump out as being highly influential predictors without many strong correlations, suggesting that their influence is accurately portrayed. Many of the time series autocorrelation features are correlated, so it may be of interest to weed out some of these predictors.

As discussed, there are several potential improvements we could make to our modeling strategy. Another consideration is to subset the Google Trends data to include only the years in which actors were actively involved in significant projects. In other words, rather than using all available trend data for each actor across all years, it may be more meaningful to focus on periods when they were actually working on major films. This would allow us to test whether the model can predict Oscar nominees specifically among actors with prominent, high-profile roles, rather than from the broader set of all actors regardless of activity.

In any case, the model as it stands lends evidence to the conclusion that Oscar buzz is a real phenomenon we can see in the data, and it does have some predictive power as to determining whether or an actor will be nominated. 


[^1]: L. Kaufman and P.J. Rousseeuw "Finding groups in data. An introduction to cluster analysis." Wiley, New York. 1990.
[^2]: R. Hyndman, Y. Kang, P., Montero-Manso, M. O'Hara-Wild, T. Talagala, E. Wang, and Y. Yang. "tsfeatures: Time Series Feature Extraction (Version 1.1.1)" Accessed: May 4, 2025. [Online]. Available https://pkg.robjhyndman.com/tsfeatures/
